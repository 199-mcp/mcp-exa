# Token Management & Context Engineering Guide

**Version**: 2.0
**Last Updated**: 2025-11-08
**Context Engineering Framework**: Based on arXiv 2510.26493v1

---

## üéØ Problem This Solves

**Before**: Exa MCP server returned 20,000-30,000 tokens per search, causing Claude to:
- Hit 32K output token limit
- Generate overly long responses
- Waste context on unnecessary content
- Fail mid-response with "exceeded maximum output tokens" errors

**After**: Token-aware MCP server with progressive disclosure:
- Smart defaults prevent overflow
- Responses include token estimates
- Progressive disclosure via caching
- Multiple content levels for different use cases
- ~80% token reduction for typical workflows

---

## üèóÔ∏è Architecture Overview

### Core Principles (Context Engineering 2.0)

1. **Minimal Sufficient Context**: Return only what's needed for the task
2. **Structured Over Unstructured**: JSON metadata + formatted text
3. **Progressive Disclosure**: Summary first, details on-demand
4. **Token Awareness**: Every response includes token estimates

### Components

```
src/
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ tokenEstimator.ts     # Token counting & cost estimation
‚îÇ   ‚îú‚îÄ‚îÄ resultCache.ts         # Temporary file caching (5min TTL)
‚îÇ   ‚îî‚îÄ‚îÄ responseFormatter.ts   # Smart formatting with metadata
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ webSearch.ts          # Enhanced with 3 content levels
‚îÇ   ‚îú‚îÄ‚îÄ config.ts             # Token-aware defaults
‚îÇ   ‚îî‚îÄ‚îÄ [other tools].ts      # To be migrated
```

---

## üìä Content Levels

### Summary (Default)
**Target**: ~150 tokens per result
**Use when**: Quick scanning, high-level overview, exploring many results

**Output format**:
```
[0] Article Title
URL: https://example.com
Published: 2024-01-15
Preview: First 200 characters of content...
Score: 0.85
```

**Example usage**:
```javascript
web_search({
  query: "AI safety research 2024",
  num_results: 10,
  content_level: "summary"
})
// Returns: ~1,500 tokens (10 √ó 150)
// Cost: ~$0.0045
```

### Standard
**Target**: ~500 tokens per result
**Use when**: Balanced detail, most common use case, exploring 3-5 results

**Output format**:
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
RESULT 0: Article Title
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
URL: https://example.com
Published: 2024-01-15
Author: John Doe
Score: 0.85

CONTENT:
[First 1500 characters with smart truncation...]
```

**Example usage**:
```javascript
web_search({
  query: "protein folding AlphaFold 3",
  num_results: 3,
  content_level: "standard"
})
// Returns: ~1,500 tokens (3 √ó 500)
// Cost: ~$0.0045
```

### Full
**Target**: ~1,500 tokens per result
**Use when**: Deep analysis, comprehensive research, 1-3 results only

**Output format**:
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
RESULT 0: Article Title
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
ID: exa-result-12345
URL: https://example.com
Published: 2024-01-15
Author: John Doe
Score: 0.85
Image: https://example.com/image.jpg
Favicon: https://example.com/favicon.ico

FULL CONTENT:
[Complete article text, up to 5000 characters...]
```

**Example usage**:
```javascript
web_search({
  query: "GPT-5 architecture details",
  num_results: 2,
  content_level: "full"
})
// Returns: ~3,000 tokens (2 √ó 1500)
// Cost: ~$0.009
```

---

## üì¶ Output Formats

### Markdown (Default) - Human-Readable
Best for presenting results directly to users or when you need formatted output.

```javascript
web_search({
  query: "AI research",
  output_format: "markdown" // or omit for default
})
```

Returns formatted markdown with metadata, cache instructions, and styled content.

### JSON - Code-Friendly (‚≠ê NEW)
**Best for code execution environments** - enables filtering, transformation, and data manipulation without token overhead.

```javascript
web_search({
  query: "AI research papers 2024",
  output_format: "json",
  num_results: 10
})
```

Returns structured JSON:
```json
{
  "metadata": {
    "cacheId": "exa-1234567890-abc",
    "totalResults": 10,
    "tokenEstimate": 2500,
    "contentLevel": "summary"
  },
  "results": [
    {
      "id": "...",
      "title": "Paper Title",
      "url": "https://...",
      "publishedDate": "2024-01-15",
      "author": "...",
      "text": "...",
      "score": 0.95
    }
  ]
}
```

### When to Use JSON Format

‚úÖ **Use JSON when**:
- Filtering results (by date, score, domain, etc.)
- Transforming data (map, reduce, aggregate)
- Working in code execution environment
- Chaining multiple operations
- Saving results to files

üìä **Token Efficiency Example**:
```javascript
// Scenario: Find 2024 papers from 20 results

// ‚ùå Markdown: ~10,000 tokens (all results in context)
const response = await web_search({
  query: "AI papers",
  num_results: 20,
  output_format: "markdown"
});
// Claude reads all 20, filters in response

// ‚úÖ JSON: ~2,000 tokens (filtered results only)
const response = await web_search({
  query: "AI papers",
  num_results: 20,
  output_format: "json"
});
const data = JSON.parse(response);
const recent = data.results.filter(r =>
  new Date(r.publishedDate) > new Date('2024-01-01')
);
// Only 5 matching results enter Claude's context

// Result: 80% token reduction!
```

**Code Execution Examples**:

```javascript
// Example 1: Score-based filtering
const data = JSON.parse(await web_search({
  query: "transformers",
  output_format: "json",
  num_results: 15
}));

const highQuality = data.results
  .filter(r => r.score > 0.9)
  .sort((a, b) => b.score - a.score);

// Example 2: Domain aggregation
const byDomain = data.results.reduce((acc, r) => {
  const domain = new URL(r.url).hostname;
  acc[domain] = (acc[domain] || 0) + 1;
  return acc;
}, {});

// Example 3: Custom transformation
const summaries = data.results.map(r => ({
  title: r.title,
  date: r.publishedDate,
  snippet: r.text.substring(0, 200)
}));
```

See `JSON_OUTPUT_EXAMPLES.md` for comprehensive examples and patterns.

---

## üîÑ Progressive Disclosure Pattern

### The Problem
Loading all results in 'full' mode wastes tokens:
```javascript
// ‚ùå INEFFICIENT
web_search({
  query: "AI papers",
  num_results: 10,
  content_level: "full"
})
// Returns: ~15,000 tokens (10 √ó 1500)
// Cost: ~$0.045
// Problem: You probably don't need ALL 10 in full detail
```

### The Solution
Use progressive disclosure:
```javascript
// ‚úÖ EFFICIENT

// Step 1: Get overview (summary mode)
const response = web_search({
  query: "AI papers",
  num_results: 10,
  content_level: "summary"
})
// Returns: ~1,500 tokens (10 √ó 150)
// Cost: ~$0.0045
// Response includes: cache_id = "exa-1699564234-abc123"

// Step 2: Identify interesting results
// User/Claude identifies: "Result [2] and [7] look relevant"

// Step 3: Get full content for specific results
const result2 = get_search_result({
  cache_id: "exa-1699564234-abc123",
  result_index: 2
})
// Returns: ~1,500 tokens (1 result, full content)
// Cost: ~$0.0045

const result7 = get_search_result({
  cache_id: "exa-1699564234-abc123",
  result_index: 7
})
// Returns: ~1,500 tokens
// Cost: ~$0.0045

// Total: ~4,500 tokens vs. 15,000 tokens
// Savings: 70% tokens, 70% cost
```

---

## üí∞ Token Budgets & Cost Estimates

### Pricing Reference (Claude Sonnet 3.5)
- Input tokens: $3.00 per 1M tokens
- Output tokens: $15.00 per 1M tokens

### Cost by Content Level & Result Count

| Results | Summary     | Standard    | Full        |
|---------|-------------|-------------|-------------|
| 1       | ~150 tok    | ~500 tok    | ~1,500 tok  |
|         | $0.00045    | $0.0015     | $0.0045     |
| 3       | ~450 tok    | ~1,500 tok  | ~4,500 tok  |
|         | $0.00135    | $0.0045     | $0.0135     |
| 5       | ~750 tok    | ~2,500 tok  | ~7,500 tok  |
|         | $0.00225    | $0.0075     | $0.0225     |
| 10      | ~1,500 tok  | ~5,000 tok  | ~15,000 tok |
|         | $0.0045     | $0.015      | $0.045      |

**Note**: These are INPUT token costs only. Claude's OUTPUT tokens cost 5√ó more.

### Hidden Token Waste

**Before**: Claude processes 15K tokens ‚Üí generates 10K token response analyzing everything
- Input: 15,000 √ó $3/M = $0.045
- Output: 10,000 √ó $15/M = $0.15
- **Total: $0.195 per search**

**After**: Claude processes 1.5K tokens ‚Üí generates 2K token focused response
- Input: 1,500 √ó $3/M = $0.0045
- Output: 2,000 √ó $15/M = $0.03
- **Total: $0.0345 per search**

**Savings: 82% cost reduction**

---

## üß† Response Metadata

Every response includes metadata to help Claude manage context:

```
üìä RESPONSE METADATA
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Results: 3/3 (complete)
Content Level: STANDARD
Token Estimate: ~1,523 tokens
Characters: 6,092
Cost Estimate: ~$0.0046 (input only)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

üîñ CACHE ID: exa-1699564234-abc123

üìö PROGRESSIVE DISCLOSURE:
‚Ä¢ To get specific result: Use cache ID + result index [0-2]
‚Ä¢ To get full content: Request with contentLevel='full'
‚Ä¢ Cache expires in: 5 minutes

üí° CONTEXT MANAGEMENT TIPS:
‚Ä¢ Current response: ~1,523 tokens
‚Ä¢ To reduce token usage: Use 'summary' level first
‚Ä¢ To increase detail: Use 'full' level for comprehensive analysis
```

### Metadata Fields

```typescript
interface ResponseMetadata {
  totalResults: number;        // Total results found
  returnedResults: number;     // Results in this response
  tokenEstimate: {
    characters: number;
    estimatedTokens: number;
    words: number;
  };
  truncated: boolean;          // Was content truncated?
  hasMore: boolean;            // More results available?
  contentLevel: 'summary' | 'standard' | 'full';
  costEstimate: {
    inputTokens: number;
    estimatedCostUSD: number;  // Input cost only
  };
}
```

---

## üéÆ Usage Patterns & Best Practices

### Pattern 1: Quick Research (High-Level Scan)

**Use case**: "What's happening with AI regulation in 2024?"

```javascript
// Step 1: Get broad overview
web_search({
  query: "AI regulation 2024",
  num_results: 10,
  content_level: "summary"
})
// ~1,500 tokens ‚Üí Scan titles and snippets

// Step 2: Claude identifies 2-3 most relevant
// Step 3: Get full content for those specific results
get_search_result({ cache_id: "...", result_index: 3 })
get_search_result({ cache_id: "...", result_index: 7 })
// ~3,000 tokens total

// Total: ~4,500 tokens vs. 15,000 tokens for full upfront
```

### Pattern 2: Focused Deep Dive

**Use case**: "Deep analysis of AlphaFold 3 paper"

```javascript
// Single result, full content
web_search({
  query: "AlphaFold 3 Nature paper 2024",
  num_results: 1,
  content_level: "full"
})
// ~1,500 tokens ‚Üí Complete analysis
```

### Pattern 3: Comparative Research

**Use case**: "Compare 5 approaches to RAG optimization"

```javascript
// Step 1: Get all approaches (summary)
web_search({
  query: "RAG optimization techniques 2024",
  num_results: 5,
  content_level: "summary"
})
// ~750 tokens ‚Üí See all options

// Step 2: Get standard detail for comparison
web_search({
  query: "RAG optimization techniques 2024",
  num_results: 5,
  content_level: "standard"
})
// ~2,500 tokens ‚Üí Enough to compare

// Alternative: Get summary, then full content for top 2
// ~750 + ~3,000 = ~3,750 tokens (saves 1,250 tokens)
```

### Pattern 4: Research + Citation

**Use case**: "Write report with citations"

```javascript
// Step 1: Summary scan for 10 sources
const overview = web_search({
  query: "mitochondrial dysfunction aging",
  num_results: 10,
  content_level: "summary"
})
// ~1,500 tokens ‚Üí Identify best sources

// Step 2: Get full content for top 5 to cite
const sources = [0, 2, 5, 7, 9].map(index =>
  get_search_result({
    cache_id: overview.cache_id,
    result_index: index
  })
)
// ~7,500 tokens ‚Üí Deep citation material

// Total: ~9,000 tokens vs. 15,000 for all full upfront
```

---

## ‚ö†Ô∏è Avoiding Token Overflow

### Common Mistakes

#### ‚ùå Mistake 1: Too many results in 'full' mode
```javascript
web_search({
  query: "AI research",
  num_results: 10,
  content_level: "full"
})
// 15,000 tokens ‚Üí Likely causes overflow in Claude's response
```

**Fix**:
```javascript
web_search({
  query: "AI research",
  num_results: 10,
  content_level: "summary"  // Start with summary
})
// 1,500 tokens ‚Üí Safe, then request specific results
```

#### ‚ùå Mistake 2: Ignoring metadata
```javascript
// Claude receives 10K tokens but doesn't check metadata
// Tries to process everything ‚Üí generates 25K token response ‚Üí overflow
```

**Fix**: Claude should:
1. Read metadata token estimate
2. Decide if it needs all content or just key results
3. Use progressive disclosure if needed

#### ‚ùå Mistake 3: Not using cache for follow-ups
```javascript
// First search
web_search({ query: "topic", content_level: "summary" })

// Want more detail ‚Üí makes NEW search in full mode
web_search({ query: "topic", content_level: "full" })
// Wastes tokens, loses cache benefit
```

**Fix**:
```javascript
// First search
const result1 = web_search({
  query: "topic",
  content_level: "summary"
})
// Note cache_id from result1

// Get specific results from cache
get_search_result({
  cache_id: result1.cache_id,
  result_index: 2
})
```

### Safety Limits

The MCP server enforces:
- **Max total tokens per response**: 25,000 (leaves 7K buffer before 32K limit)
- **Smart max_characters calculation**: Based on `num_results` to stay under budget
- **Cache expiry**: 5 minutes (prevents stale data, limits storage)

---

## üìà Performance Metrics

### Token Reduction

| Scenario                  | Before (Old) | After (New) | Reduction |
|---------------------------|--------------|-------------|-----------|
| Quick scan (10 results)   | 25,000 tok   | 1,500 tok   | **94%**   |
| Balanced research (5)     | 18,000 tok   | 2,500 tok   | **86%**   |
| Deep dive (3)             | 12,000 tok   | 4,500 tok   | **62%**   |
| Progressive (10‚Üí3 full)   | 25,000 tok   | 6,000 tok   | **76%**   |

### Cost Reduction

| Workflow                           | Before      | After       | Savings |
|------------------------------------|-------------|-------------|---------|
| Research + analysis (10 results)   | $0.195      | $0.035      | **82%** |
| Multiple queries (5√ó per session)  | $0.975      | $0.175      | **82%** |
| Daily usage (20 queries)           | $3.90       | $0.70       | **82%** |

---

## üõ†Ô∏è Implementation Details

### Token Estimation

Uses industry-standard approximation:
```
tokens ‚âà characters / 4
```

**Accuracy**: ¬±5% for typical web content (validated against OpenAI tokenizer)

### Caching Strategy

**Storage**: System temp directory (`/tmp/exa-mcp-cache/`)
**Format**: JSON files
**TTL**: 5 minutes (refreshed on access)
**Cleanup**: Automatic on server start + periodic
**Max cache size**: 100 searches

**File structure**:
```json
{
  "cacheId": "exa-1699564234-abc123",
  "query": "AI safety",
  "timestamp": 1699564234000,
  "ttl": 300000,
  "totalResults": 5,
  "results": [...],
  "metadata": {
    "requestId": "req-xyz",
    "searchType": "neural"
  }
}
```

### Smart Max Characters Calculation

```typescript
function calculateMaxCharacters(
  numResults: number,
  targetTotalTokens: number = 20000
): number {
  const metadataOverhead = 1000; // tokens for formatting
  const availableTokens = targetTotalTokens - metadataOverhead;
  const tokensPerResult = Math.floor(availableTokens / numResults);
  const charsPerResult = tokensPerResult * 4;

  return Math.max(500, Math.min(charsPerResult, 5000));
}
```

**Examples**:
- `numResults = 3` ‚Üí `maxChars = 6,333` (clamped to 5,000)
- `numResults = 5` ‚Üí `maxChars = 3,800`
- `numResults = 10` ‚Üí `maxChars = 1,900`

---

## üß™ Testing & Validation

### Test Token Estimates

```bash
# Run web search with token tracking
npm run build
node -e "
const { estimateTokens } = require('./dist/utils/tokenEstimator.js');
const testText = 'Your search results here...';
console.log(estimateTokens(testText));
"
```

### Test Progressive Disclosure

```javascript
// Test caching
const result1 = await web_search({
  query: "test",
  content_level: "summary"
});

// Extract cache_id from result
const cacheId = extractCacheId(result1);

// Test retrieval
const result2 = await get_search_result({
  cache_id: cacheId,
  result_index: 0
});

// Verify result2 has full content
```

### Validate Token Limits

```bash
# Test with various configurations
for content_level in summary standard full; do
  for num in 1 3 5 10; do
    echo "Testing: $num results, $content_level mode"
    # Make request, check response token count
  done
done
```

---

## üéì For Claude Skills / Agents

### Integration in SKILL.md

```markdown
<!-- CACHED CONTEXT START -->

## Tool Usage: Exa Web Search

**Token-aware patterns**:
1. Use `content_level='summary'` for initial exploration
2. Monitor token estimates in responses
3. Use progressive disclosure for large result sets
4. Cache ID valid for 5 minutes

**Example workflow**:
```javascript
// Step 1: Overview
const overview = web_search({
  query: userQuery,
  num_results: 10,
  content_level: "summary"
});

// Step 2: Analyze summaries, pick top 3
const topIndices = analyzeAndRank(overview.results);

// Step 3: Get full content for top 3
const detailedResults = topIndices.map(i =>
  get_search_result({
    cache_id: overview.cache_id,
    result_index: i
  })
);
```

<!-- CACHED CONTEXT END -->
```

### Ultrathink Integration

When using ultrathink mode:
1. **Planning phase**: Use `content_level='summary'` to gather breadth
2. **Deep thinking**: Request specific results via `get_search_result`
3. **Synthesis**: Claude has focused context, not overwhelmed

---

## üìö References

- **Context Engineering 2.0**: arXiv:2510.26493v1
- **Prompt Caching**: Claude's 90% cost reduction for static content
- **Progressive Disclosure**: Load on-demand, not upfront
- **Token Estimation**: OpenAI tokenizer approximation (chars/4)

---

## üîÑ Migration from v1

### Breaking Changes

1. **Default `num_results`**: 5 ‚Üí 3
2. **Default `max_characters`**: 3000 ‚Üí 1500
3. **Response format**: Raw JSON ‚Üí Formatted text with metadata

### Migration Steps

1. **Update tool calls**:
   ```javascript
   // Old
   web_search({ query, numResults: 5 })

   // New
   web_search({
     query,
     num_results: 5,
     content_level: "standard"  // Explicit level
   })
   ```

2. **Handle new response format**:
   ```javascript
   // Old: response.data.results
   // New: Formatted text with metadata header
   ```

3. **Use progressive disclosure**:
   ```javascript
   // Old: Get all 10 results in full
   // New: Get summary, then request specific results
   ```

### Backwards Compatibility

To use old behavior (NOT RECOMMENDED):
```javascript
web_search({
  query,
  num_results: 5,
  content_level: "full",
  max_chars_per_result: 3000
})
// Mimics old behavior, but WILL cause token overflow
```

---

## üêõ Troubleshooting

### Issue: "Cache not found or expired"

**Cause**: Cache TTL is 5 minutes
**Fix**: Re-run `web_search`, caches are temporary by design

### Issue: "Still hitting 32K limit"

**Causes**:
1. Using `content_level='full'` with `num_results > 5`
2. Claude generating very long analysis

**Fixes**:
1. Reduce `num_results` or use `content_level='summary'`
2. Configure `CLAUDE_CODE_MAX_OUTPUT_TOKENS=25000`

### Issue: "Token estimates seem off"

**Cause**: Different content types have different token densities
**Fix**: Token estimation uses conservative average (chars/4), may vary ¬±20% for code-heavy or multilingual content

---

## üìû Support

**GitHub**: https://github.com/199-biotechnologies/exa-mcp-server
**Issues**: Report token overflow cases with query + config
**Docs**: This file + `CLAUDE.md` + inline code comments
